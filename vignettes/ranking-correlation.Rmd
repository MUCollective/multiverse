---
title: "Ranking Visualizations of Correlation from JNDs"
author: "Alex Kale"
date: "8/26/2020"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)        # ggplot, stat_..., geom_..., etc
library(magrittr)       # %>%, %<>%
library(dplyr)          # filter, rename, mutate, group_by, ungroup, ...
library(purrr)          # map2
library(tidyr)          # unnest
library(lme4)
library(quickpsy)       # quickpsy
library(emmeans)
library(tidybayes)
# library(multiverse)

knitr::opts_chunk$set(echo = TRUE)
```

## Multiverse analysis ranking visualizations of correlation by just-noticeable differences

In this multiverse analysis, we reanalyze data from Lane Harrison and colleagues' paper, [Ranking Visualizations of Correlation Using Weberâ€™s Law](https://visualthinking.psych.northwestern.edu/publications/Harrison-weberlaw-infovis2014.pdf). The code in this document was adapted from [supplemental materials](https://github.com/mjskay/ranking-correlation/blob/master/README.md) from Matthew Kay and Jeff Heer's paper, [Beyond Weber's Law: A Second Look at Ranking Visualizations of Correlation](https://idl.cs.washington.edu/files/2015-BeyondWebersLaw-InfoVis.pdf), which reanalyzed the data from the original study using a log-linear Bayesian model and fewer exclusion criteria. 

Based on these two studies, we've identified the following *decision alternatives* to test in our multiverse analysis.
 - Operationalizing JNDs
 - Handling performance at or worse than chance
 - Handling non-normality in the distribution of JNDs
 - Adjusting for bias due to the direction of approach in the staircase sampling procedure
 - Whether or not we include random effects per participant in our inferential model

We start by declaring a multiverse object to place our data analysis inside of.

```{r}
M = multiverse()
```

First, we load the data and change some variable names.

```{multiverse, data, inside = M}
# data("vis_correlation_all_judgments")
load("../data/vis_correlation_all_judgments.rda")

df <- vis_correlation_all_judgments %>%
  # one column for visualization condition and correlation direction
  unite("visandsign", vis:rdirection) %>%
  # rename correlation manipulations, and define difference (dr) between target correlation (r) and manipulated correlation (mr)
  rename(
    r = rbase,
    mr = rv,
    correct = gotItRight
  ) %>%
  # set up variables for modeling
  mutate(
    dr = round(abs(mr - r) * 100) / 100,
    participant = as.factor(participant)
  ) %>%
  # remove pre-computed JNDs; we will calculate our own JNDs
  dplyr::select(-jnd) %>% 
  # select handful of participants for debugging
  filter(participant %in% c("hrqfen7i", "hs1tlxtw", "hskjvwol", "hsyws9iw", "hsi8loaf")) 
```

Next we operationalize just-noticeable differences (JNDs), the dependent variable in our analysis. This is a major decision point where we will multiplex across multiple common strategies.
 - Derive JNDs by assuming staircase convergence, taking the average stimulus intensity over the last 24 trials as in Harrison et al.'s original analysis.
 - Fit logistic regressions to the data from each individual staircase, and derive JNDs as the level of stimulus intensity where the curve predicts 75% accuracy.
 - Fit scaled and shifted cumulative Gaussian psychometric functions (as in [Kale et al. 2018](http://users.eecs.northwestern.edu/~jhullman/hops_jobs_pfs.pdf) to the data from each individual staircase, and derive JNDs as the level of stimulus intensity where the curve predicts 75% accuracy.
 - Fit a big hierarchical model to all judgments and derive JNDs from the joint model fit.

Two of these strategies require a function to estimate JNDs from the slope and intercept of logistic regression.
 
```{r}
# derive JND from logistic curve
jnd_from_logistic <- function(intercept, slope) {
  return((qlogis(0.75) - intercept) / slope)
}
```

```{multiverse, jnds, inside = M}
# operationalize just-noticeable differences (jnd)
derive_jnd <- branch(operationalize_jnd,
  "staircase_convergence" ~ function(df) {
    df <- df %>%
      group_by(participant, r, approach) %>%
      filter(index > (max(index) - 24)) %>%
      mutate(jnd = mean(dr)) %>%
      ungroup() %>%
      dplyr::select(-index)
    
    return(df)
  },
  "logistic_curve_fitting" ~ function(df) {
    df <- df %>%
      group_by(participant, r, approach, visandsign) %>%
      # TODO: fix summarise bug/warning. wtf is going on with this?
      summarise(
        dr = list(dr),
        correct = list(as.integer(correct)),
      ) %>%
      mutate(
        # logistic regression with log stimulus intensity as predictor
        fit = map2(dr, correct, ~glm(.y ~ log(.x), family = binomial)), 
        b_0 = unlist(map(fit, ~coef(.x)[[1]])),
        b_1 = unlist(map(fit, ~coef(.x)[[2]])),
        # fit = map2(dr, correct, ~gamlss(.y ~ log(.x), family = BI)), 
        # b_0 = param_from_gamlss(fit, "intercept"),
        # b_1 = param_from_gamlss(fit, "slope"),
        jnd = exp(jnd_from_logistic(b_0, b_1))
      ) %>%
      unnest(cols = c(dr, correct)) %>%
      ungroup()
    
    return(df)
  },
  "guess_lapse_curve_fitting" ~ function(df) {
    # prepare data for psychometeric function fitting
    psy_data <- df %>%
      group_by(participant, r, approach, visandsign, dr) %>%
      # TODO: fix summarise bug/warning. wtf is going on with this?
      summarise(
        # number correct and number of trials at each level of stimulus intensity for each staircase
        k = sum(as.integer(correct)),
        n = n() 
      ) 
    
    # fit a scaled and shifted cumulative Gaussian psychometric function with 
    # guess rate of 0.5, lapse rate as a free parameter (as in Kale et al. 2019)
    fit <- quickpsy(psy_data, dr, k, n, grouping = .(participant, r, approach, visandsign), bootstrap = "none", guess = 0.5, lapses = TRUE, log = TRUE, prob = 0.75, parini = c(0.2, 0.5, .01))
    
    # join results with original dataframe
    df <- fit$thresholds %>%
      group_by(participant, r, approach, visandsign) %>%
      rename(jnd = thre) %>%
      full_join(df, by = c("participant", "r", "approach", "visandsign")) %>%
      ungroup() %>%
      # this approach sometimes fails generating NA values for jnd that we need to drop
      drop_na()
    
    return(df)
  }#,
  # TODO: add hierarchical path
  # "hierarchical" ~ { # derive jnds after model fitting
  #   df
  # } 
)

df <- derive_jnd(df)
```
  
Here, we create and apply a logical index for exclusion criteria. First, we handle chance performance by excluding entire `visandsign` conditions where there are too many staircases with performance worse than chance (as in Harrison et al.). Kay and Heer addressed this issue by censoring the data, but we were not able to get censored models to converge for our analysis. Thus, we leave this decision out.

Second, we make a decision about how to handle the fact that JNDs are not normally distributed. Do we exclude outliers (as in Harrison et al.) or log-transform our JNDs later on (as in Kay and Heer)?
Exclusions will need to be handled later for the hierarchical model where we haven't yet defined JNDs at this point in the analysis.

```{multiverse, exclusions, inside = M}
# calculate values used to derive filtering conditions used in original paper
df <- df %>%
  # exclude visandsign with > 20% jnds worse than chance performance (> 0.45)
  group_by(visandsign) %>%
  mutate(
    p_chance_cutoff = branch(handle_performance_at_chance,
      "exclude" %when% (operationalize_jnd != "hierarchical") ~ mean(jnd > .45) > 0.2 
    )
  ) %>%
  group_by(visandsign, r, approach) %>%
  mutate(
    mad_cutoff = branch(handle_non_normality,
      "exclude" %when% (operationalize_jnd != "hierarchical") ~ abs(jnd - median(jnd)) > 3 * mad(jnd), # exclude observations > 3 median-absolute deviations from the median within each group
      "log_transform" ~ FALSE # don't exclude any data, log transform instead
    )
  ) %>%
  ungroup() %>%
  # aggregate exclusion criteria
  mutate(exclude = p_chance_cutoff | mad_cutoff) %>%
  filter(!exclude)
```

Next, we make a decision about how we code the direction of approach for the staircase. We will use this to correct for bias later.
 - In the original study by Harrison et al., they define an adjusted correlation value to use in their model, as in [Rensink and Baldrigde (2010)](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1467-8659.2009.01694.x).
 - In Kay and Heer's reanalysis, they code `approach` so that other coefficients can be interpreted as relative to the mean of both approaches. They first relevel so that "below" == 1 and "above" == -1 (eliminating a double-negative so that the sign of the coefficient of approach is positive, making interpretation simpler). Then gthey make a numeric version of the approach coded as sum-to-zero (this is easier to work with than a factor in many cases, for example if we want a model where we can make unbiased predictions at `approach = 0`).

```{multiverse, adjust-approach, inside = M}
adjustment <- branch(adjust_for_approach_bias,
  "adjusted_correlation" ~ function(df) {
    df<- df %>%
      group_by(visandsign, r, approach) %>%
      mutate(mean_jnd_per_approach = mean(jnd)) %>%
      group_by(visandsign, r) %>%
      mutate(
        mean_jnd_within_r = mean(mean_jnd_per_approach),
        r = r + ifelse(approach == "above", 0.5, -0.5) * mean_jnd_within_r # adjusted correlations
      ) %>% 
      ungroup()
    
    return(df)
  },
  "approach_as_covariate" ~ function(df) {
    df$approach = ordered(df$approach, levels = c("above", "below"))
    contrasts(df$approach) = contr.sum
    
    df <- df %>%
      mutate(approach_value = if_else(approach == "above", -1, 1))
    
    return(df)
  }
)

df <- adjustment(df)
```



Now, we are ready to model our data. Multiple decisions impact the implementation here, so we use multiple branches to represent the impacts of *decisions we have already made* at the modeling stage. The only new decision we introduce at this point is whether or not to include random intercepts per participant in our model specification.

```{multiverse, modeling, inside = M}
m <- branch(random_effects,
  "yes" %when% (operationalize_jnd != "hierarchical") ~
    branch(handle_non_normality,
      "exclude" ~ # linear model
        lmer(jnd ~ r * visandsign
            * branch(adjust_for_approach_bias,
                "adjusted_correlation" ~ NULL,
                "approach_as_covariate" ~ approach_value
              )
            + (1 | participant),
          data = df
        ),
      "log_transform" ~ # log-linear model
        glmer(jnd ~ r * visandsign
            * branch(adjust_for_approach_bias, # conditions avoid fit issues in specific universes
                "adjusted_correlation" ~ NULL,
                # AMK: condition on next line should cut 2 universes, but cuts 8. what is happening?
                "approach_as_covariate" %when% (operationalize_jnd == "guess_lapse_curve_fitting") ~ approach_value
              )
            + (1 | participant),
          data = df,
          family = gaussian(link = "log")
        )
    ),
  "no" %when% (operationalize_jnd != "hierarchical") ~ 
    branch(handle_non_normality,
      "exclude" ~ # linear model
        lm(jnd ~ r * visandsign
            * branch(adjust_for_approach_bias,
                "adjusted_correlation" ~ NULL,
                "approach_as_covariate" ~ approach_value
              ),
          data = df
        ),
      "log_transform" ~ # log-linear model
        glm(jnd ~ r * visandsign
            * branch(adjust_for_approach_bias,
                "adjusted_correlation" ~ NULL,
                "approach_as_covariate" ~ approach_value
              ),
          data = df,
          family = gaussian(link = "log")
        )
    )#,
  # TODO: add hierarchical path
  # "hierarchical" %when% (operationalize_jnd == "hierarchical") ~ # logistic regression and derive JNDs after 
    # glmer(correct ~ r * log(dr) * visandsign
    #     * branch(adjust_for_approach_bias,
    #         "adjusted_correlation" ~ NULL,
    #         "approach_as_covariate" ~ approach_value
    #       )
    #     + (1 | participant),
    #   data = df,
    #   family = binomial
    # )
)
```

```{r}
execute_multiverse(M)
```
```{r}
expand(M)
```

```{r}
attr(M, "multiverse")$multiverse_diction$get("adjust-approach")[[9]]
```

```{r}
attr(M, "multiverse")$multiverse_diction$get("modeling")[[11]]
```


```{r}
expand(M)$.code[[11]]
```


Now we postprocess our models. The way we do this depends on *decisions we've already made* about how to derive JNDs and adjust for approach bias. The primary branch below is that when we derive JNDs before fitting an inferential model, we just need to extract and aggregate model predictions, whereas when we fit a hierarchical logistic regression to all the data, we need to derive JNDs from this model fit after fitting the model.

```{multiverse, postprocessing, inside = M}
# AMK: why is there an unexpected * in the model formula when I call emmeans?
postprocess <- branch(operationalize_jnd,
    "staircase_convergence" ~ function(m) {
      # extract marginal estimate for JNDs in each level of visandsign
      output <- m %>% 
        emmeans(~ visandsign, 
          cov.keep = c("r"), 
          type = "response"
        ) %>% 
        summary() #%>% # to dataframe
        # TODO: renaming depends on modeling function
        # rename(
        #   jnd = response,
        #   lowerCI = asymp.LCL,
        #   upperCI = asymp.UCL
        # ) %>%
        # dplyr::select(-df)

      return(output)
    },
    "logistic_curve_fitting" ~ function(m) {
      # extract marginal estimate for JNDs in each level of visandsign
      output <- m %>% 
        emmeans(~ visandsign, 
          cov.keep = c("r"), 
          type = "response"
        ) %>% 
        summary() #%>% # to dataframe
        # TODO: renaming depends on modeling function
        # rename(
        #   jnd = response,
        #   lowerCI = asymp.LCL,
        #   upperCI = asymp.UCL
        # ) %>%
        # dplyr::select(-df)

      return(output)
    },
    "guess_lapse_curve_fitting" ~ function(m) {
      # extract marginal estimate for JNDs in each level of visandsign
      output <- m %>% 
        emmeans(~ visandsign, 
          cov.keep = c("r"), 
          type = "response"
        ) %>% 
        summary() #%>% # to dataframe
        # TODO: renaming depends on modeling function
        # rename(
        #   jnd = response,
        #   lowerCI = asymp.LCL,
        #   upperCI = asymp.UCL
        # ) %>%
        # dplyr::select(-df)

      return(output)
    }#,
    # TODO: revise postprocessing for hierarchical path to use emmeans()
    # "hierarchical" ~ function(m, df) { 
    #   # extract predictions for probability of answering correctly
    #   predictions <- expand.grid(r = unique(df$r), dr = exp(c(0, 1)), visandsign = levels(factor(df$visandsign)), participant = levels(factor(df$participant))) %>%
    #     branch(adjust_for_approach_bias,
    #       "adjusted_correlation" ~ (function(d) { return(d) }), # do nothing
    #       "approach_as_covariate" ~ mutate(approach_value = 0)
    #     ) %>%
    #     cbind(prediction = predict(m, se.fit = TRUE, newdata = ., data = df, type = "response"))
    #
    #   # use predictions at stimulus intensities of 0 and 1 (exponentiated) to calculate slope and intercept
    #   slope_df <- predictions %>%
    #     group_by(r, dr, visandsign, participant) %>%
    #     compare_levels(prediction, by = dr) %>%
    #     rename(b_1 = prediction) %>%
    #     dplyr::select(-dr)
    #   intercept_df <- predictions %>%
    #     filter(dr == exp(0)) %>%
    #     rename(b_0 = prediction) %>%
    #     dplyr::select(-dr)
    #
    #   output <- slope_df %>%
    #     full_join(intercept_df, by = c("r", "visandsign", "participant")) %>%
    #     mutate(
    #       jnd = exp(jnd_from_logistic(b_0, b_1)) # finally define JNDs
    #     ) %>%
    #
    #     # apply exclusion criteria to JNDs now that we've defined them in this branch:
    #     # exclude visandsign with > 20% jnds worse than chance performance (> 0.45)
    #     group_by(visandsign) %>%
    #     mutate(p_chance_cutoff = mean(jnd > .45) > 0.2) %>%
    #     group_by(visandsign, r, approach) %>%
    #     mutate(
    #       mad_cutoff = branch(handle_non_normality,
    #         "exclude" ~ abs(jnd - median(jnd)) > 3 * mad(jnd), # exclude observations > 3 median-absolute deviations from the median within each group
    #         "log_transform" ~ FALSE # don't exclude any data, log transform instead
    #       )
    #     ) %>%
    #     ungroup() %>%
    #     # aggregate exclusion criteria
    #     mutate(exclude = p_chance_cutoff | mad_cutoff) %>%
    #     filter(!exclude) %>%
    #
    #     # aggregate JND estimates to output format
    #     group_by(r, visandsign) %>%
    #     median_qi(jnd)
    #
    #   return(output)
    # }
)

output <- postprocess(m)
```

```{r}
# AMK: "crossed wires" issue causes errors in universes 11 and 12 (see the errors below on execution)
  # error in universe 11
  # Error in eval(predvars, data, env): object 'approach_value' not found
  # error in universe 12
  # Error in eval(predvars, data, env): object 'approach_value' not found
execute_multiverse(M)
```

## Debugging

```{r}
M_expand <- multiverse::expand(M)
```

conditions

```{r}
# AMK: re %when% conditions; the conditions in the modeling block should omit 2 of 24 universes, but we can see here that there are 16 (8 have been omitted, why?)
M_expand %>% summarise(n())
```

"crossed wires"

```{r}
# AMK: re "crossed wires"; we cans see that universes 11 and 12 should adjust_for_approach_bias via the approach_as_covariate path...
M_expand %>% filter(.universe %in% 11:12)
```

```{r}
# AMK: yet we cans see in universe 11 that the generated .code block is taking the opposite path, adjusted_correlation. note that approach_value is not defined, but r is mutated...
M_expand$.code[[11]]$`adjust-approach`
```

```{r}
# AMK: and universe 12 is doing the same. "crossed wires" is still and issue
M_expand$.code[[12]]$`adjust-approach`
```

extra * in model formula

```{r}
# AMK: re unexpected * in the model formula; taking universe 5 for example, we can see that the modeling block looks well formed...
M_expand$.code[[5]]$modeling
```

```{r}
# AMK: but when we run the model, the formula is all messed up. This doesn't happen in every universe. There are also fit issues related to the random effect variance getting stuck at zero. Could this be related to the model spec getting messed up? I'm thinking of omitting models with random effects from the universe since most of them seem to have convergence issues with only 4 JND estimates per participant.
m <- lmer(jnd ~ r * visandsign * NULL + (1 | participant), data = df)
summary(m)
```

don't worry about this, Abhraneel

```{r}
# TODO: figure out why models don't converge when log_transform, approach_as_covariate, and random_effects==yes (e.g., in universe 15)
M_expand$.code[[15]]
```

```{r}
# M_expand$.results[[5]]$df
```

```{r}
# saveRDS(M, file = "ranking-correlation.rds")
# readRDS("ranking-correlation.rds")
```

## Visualize Results

```{r}
# define marginal CIs before we get to this point so that we don't have to extract full dataframes
# results <- extract_variables(M, output)
```

